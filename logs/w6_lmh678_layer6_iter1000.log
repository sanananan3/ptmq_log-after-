 <class 'models.resnet.ResNet'>
conv1 <class 'quant.quant_module.QuantizedLayer'>
conv1.module <class 'quant.quant_module.QConv2d'>
conv1.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
conv1.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
conv1.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
conv1.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
conv1.activation <class 'torch.nn.modules.activation.ReLU'>
bn1 <class 'utils.fold_bn.StraightThrough'>
relu <class 'utils.fold_bn.StraightThrough'>
maxpool <class 'torch.nn.modules.pooling.MaxPool2d'>
layer1 <class 'torch.nn.modules.container.Sequential'>
layer1.0 <class 'utils.model_utils.QuantBasicBlock'>
layer1.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer1.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer1.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer1.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer1.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer1.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer1.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer1.1 <class 'utils.model_utils.QuantBasicBlock'>
layer1.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer1.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer1.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer1.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer1.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer1.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer1.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer2 <class 'torch.nn.modules.container.Sequential'>
layer2.0 <class 'utils.model_utils.QuantBasicBlock'>
layer2.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer2.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer2.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer2.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer2.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer2.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer2.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer2.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer2.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer2.1 <class 'utils.model_utils.QuantBasicBlock'>
layer2.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer2.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer2.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer2.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer2.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer2.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer2.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer3 <class 'torch.nn.modules.container.Sequential'>
layer3.0 <class 'utils.model_utils.QuantBasicBlock'>
layer3.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer3.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer3.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer3.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer3.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer3.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer3.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer3.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer3.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer3.1 <class 'utils.model_utils.QuantBasicBlock'>
layer3.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer3.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer3.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer3.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer3.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer3.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer3.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer4 <class 'torch.nn.modules.container.Sequential'>
layer4.0 <class 'utils.model_utils.QuantBasicBlock'>
layer4.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer4.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer4.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer4.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer4.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer4.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer4.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer4.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer4.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer4.1 <class 'utils.model_utils.QuantBasicBlock'>
layer4.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer4.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer4.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer4.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer4.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer4.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer4.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
avgpool <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>
fc <class 'quant.quant_module.QuantizedLayer'>
fc.module <class 'quant.quant_module.QLinear'>
fc.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
fc.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
Starting model calibration...
Completed model calibration
Starting block reconstruction...
Iter: 0, Loss: 0.000, round_loss: 0.000, recon_loss: 0.000
Iter: 200, Loss: 80.698, round_loss: 80.698, recon_loss: 0.000
Iter: 400, Loss: 66.455, round_loss: 66.455, recon_loss: 0.000
Iter: 600, Loss: 41.929, round_loss: 41.929, recon_loss: 0.000
Iter: 800, Loss: 26.382, round_loss: 26.382, recon_loss: 0.000
Iter: 0, Loss: 0.824, round_loss: 0.000, recon_loss: 0.824
Iter: 200, Loss: 657.859, round_loss: 657.140, recon_loss: 0.718
Iter: 400, Loss: 548.636, round_loss: 547.921, recon_loss: 0.716
Iter: 600, Loss: 402.537, round_loss: 401.766, recon_loss: 0.771
Iter: 800, Loss: 314.475, round_loss: 313.799, recon_loss: 0.676
Iter: 0, Loss: 3.366, round_loss: 0.000, recon_loss: 3.366
Iter: 200, Loss: 703.472, round_loss: 700.468, recon_loss: 3.003
Iter: 400, Loss: 590.704, round_loss: 587.418, recon_loss: 3.286
Iter: 600, Loss: 446.229, round_loss: 443.204, recon_loss: 3.025
Iter: 800, Loss: 353.995, round_loss: 351.217, recon_loss: 2.778
Iter: 0, Loss: 1.214, round_loss: 0.000, recon_loss: 1.214
Iter: 200, Loss: 2182.219, round_loss: 2180.914, recon_loss: 1.305
Iter: 400, Loss: 1819.230, round_loss: 1818.174, recon_loss: 1.056
Iter: 600, Loss: 1295.979, round_loss: 1294.736, recon_loss: 1.243
Iter: 800, Loss: 981.698, round_loss: 980.423, recon_loss: 1.275
Iter: 0, Loss: 1.658, round_loss: 0.000, recon_loss: 1.658
Iter: 200, Loss: 2804.785, round_loss: 2803.216, recon_loss: 1.569
Iter: 400, Loss: 2336.624, round_loss: 2335.106, recon_loss: 1.518
Iter: 600, Loss: 1638.507, round_loss: 1636.962, recon_loss: 1.544
Iter: 800, Loss: 1226.339, round_loss: 1224.747, recon_loss: 1.592
Iter: 0, Loss: 0.658, round_loss: 0.000, recon_loss: 0.658
Iter: 200, Loss: 8729.215, round_loss: 8728.639, recon_loss: 0.576
Iter: 400, Loss: 7252.906, round_loss: 7252.371, recon_loss: 0.535
Iter: 600, Loss: 4933.808, round_loss: 4933.215, recon_loss: 0.593
Iter: 800, Loss: 3567.142, round_loss: 3566.523, recon_loss: 0.619
Iter: 0, Loss: 0.652, round_loss: 0.000, recon_loss: 0.652
Iter: 200, Loss: 11224.225, round_loss: 11223.529, recon_loss: 0.696
Iter: 400, Loss: 9329.278, round_loss: 9328.617, recon_loss: 0.661
Iter: 600, Loss: 6312.996, round_loss: 6312.240, recon_loss: 0.755
Iter: 800, Loss: 4539.167, round_loss: 4538.459, recon_loss: 0.708
Iter: 0, Loss: 0.648, round_loss: 0.000, recon_loss: 0.648
Iter: 200, Loss: 34918.801, round_loss: 34918.164, recon_loss: 0.637
Iter: 400, Loss: 29003.252, round_loss: 29002.621, recon_loss: 0.631
Iter: 600, Loss: 19595.137, round_loss: 19594.590, recon_loss: 0.548
Iter: 800, Loss: 14073.065, round_loss: 14072.422, recon_loss: 0.643
Iter: 0, Loss: 39.990, round_loss: 0.000, recon_loss: 39.990
Iter: 200, Loss: 44943.902, round_loss: 44903.727, recon_loss: 40.176
Iter: 400, Loss: 37784.238, round_loss: 37747.219, recon_loss: 37.020
Iter: 600, Loss: 29406.328, round_loss: 29373.465, recon_loss: 32.863
Iter: 800, Loss: 24137.938, round_loss: 24101.510, recon_loss: 36.427
Iter: 0, Loss: 0.029, round_loss: 0.000, recon_loss: 0.029
Iter: 200, Loss: 4870.654, round_loss: 4870.627, recon_loss: 0.027
Iter: 400, Loss: 4040.758, round_loss: 4040.729, recon_loss: 0.028
Iter: 600, Loss: 2586.550, round_loss: 2586.529, recon_loss: 0.021
Iter: 800, Loss: 1698.041, round_loss: 1698.013, recon_loss: 0.028
Completed block reconstruction
Block reconstruction took 271.638564825058 seconds
Starting model evaluation of W6A6 block reconstruction (low)...
Top-1 accuracy: 69.58, Top-5 accuracy: 88.96
Starting model evaluation of W6A7 block reconstruction (med)...
Top-1 accuracy: 69.57, Top-5 accuracy: 88.96
Starting model evaluation of W6A8 block reconstruction (high)...
Top-1 accuracy: 69.62, Top-5 accuracy: 88.95
