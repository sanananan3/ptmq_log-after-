 <class 'models.resnet.ResNet'>
conv1 <class 'quant.quant_module.QuantizedLayer'>
conv1.module <class 'quant.quant_module.QConv2d'>
conv1.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
conv1.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
conv1.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
conv1.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
conv1.activation <class 'torch.nn.modules.activation.ReLU'>
bn1 <class 'utils.fold_bn.StraightThrough'>
relu <class 'utils.fold_bn.StraightThrough'>
maxpool <class 'torch.nn.modules.pooling.MaxPool2d'>
layer1 <class 'torch.nn.modules.container.Sequential'>
layer1.0 <class 'utils.model_utils.QuantBasicBlock'>
layer1.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer1.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer1.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer1.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer1.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer1.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer1.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer1.1 <class 'utils.model_utils.QuantBasicBlock'>
layer1.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer1.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer1.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer1.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer1.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer1.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer1.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer1.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer1.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer1.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer1.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer1.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer2 <class 'torch.nn.modules.container.Sequential'>
layer2.0 <class 'utils.model_utils.QuantBasicBlock'>
layer2.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer2.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer2.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer2.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer2.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer2.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer2.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer2.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer2.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer2.1 <class 'utils.model_utils.QuantBasicBlock'>
layer2.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer2.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer2.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer2.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer2.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer2.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer2.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer2.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer2.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer2.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer2.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer2.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer3 <class 'torch.nn.modules.container.Sequential'>
layer3.0 <class 'utils.model_utils.QuantBasicBlock'>
layer3.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer3.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer3.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer3.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer3.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer3.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer3.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer3.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer3.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer3.1 <class 'utils.model_utils.QuantBasicBlock'>
layer3.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer3.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer3.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer3.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer3.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer3.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer3.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer3.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer3.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer3.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer3.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer3.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer4 <class 'torch.nn.modules.container.Sequential'>
layer4.0 <class 'utils.model_utils.QuantBasicBlock'>
layer4.0.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer4.0.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer4.0.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.0.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.0.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer4.0.conv2.module <class 'quant.quant_module.QConv2d'>
layer4.0.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.downsample <class 'quant.quant_module.QuantizedLayer'>
layer4.0.downsample.module <class 'quant.quant_module.QConv2d'>
layer4.0.downsample.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.0.downsample.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.0.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.0.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.0.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer4.0.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer4.0.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.0.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
layer4.1 <class 'utils.model_utils.QuantBasicBlock'>
layer4.1.conv1_relu <class 'quant.quant_module.QuantizedLayer'>
layer4.1.conv1_relu.module <class 'quant.quant_module.QConv2d'>
layer4.1.conv1_relu.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.1.conv1_relu.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.1.conv1_relu.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.1.conv1_relu.layer_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.conv1_relu.layer_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.1.conv2 <class 'quant.quant_module.QuantizedLayer'>
layer4.1.conv2.module <class 'quant.quant_module.QConv2d'>
layer4.1.conv2.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
layer4.1.conv2.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
layer4.1.activation <class 'torch.nn.modules.activation.ReLU'>
layer4.1.block_post_act_fake_quantize <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize.observer <class 'quant.observer.MSEObserver'>
layer4.1.block_post_act_fake_quantize_low <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize_low.observer <class 'quant.observer.MSEObserver'>
layer4.1.block_post_act_fake_quantize_med <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize_med.observer <class 'quant.observer.MSEObserver'>
layer4.1.block_post_act_fake_quantize_high <class 'quant.fake_quant.LSQFakeQuantize'>
layer4.1.block_post_act_fake_quantize_high.observer <class 'quant.observer.MSEObserver'>
avgpool <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>
fc <class 'quant.quant_module.QuantizedLayer'>
fc.module <class 'quant.quant_module.QLinear'>
fc.module.weight_fake_quant <class 'quant.fake_quant.AdaRoundFakeQuantize'>
fc.module.weight_fake_quant.observer <class 'quant.observer.MSEObserver'>
Starting model calibration...
Completed model calibration
Starting block reconstruction...
Iter: 0, Loss: 0.002, round_loss: 0.000, recon_loss: 0.002
Iter: 200, Loss: 80.683, round_loss: 80.681, recon_loss: 0.002
Iter: 400, Loss: 66.475, round_loss: 66.474, recon_loss: 0.002
Iter: 600, Loss: 42.194, round_loss: 42.193, recon_loss: 0.002
Iter: 800, Loss: 27.006, round_loss: 27.004, recon_loss: 0.001
Iter: 0, Loss: 21.692, round_loss: 0.000, recon_loss: 21.692
Iter: 200, Loss: 673.152, round_loss: 655.751, recon_loss: 17.401
Iter: 400, Loss: 583.233, round_loss: 566.578, recon_loss: 16.655
Iter: 600, Loss: 502.036, round_loss: 484.325, recon_loss: 17.711
Iter: 800, Loss: 432.863, round_loss: 416.932, recon_loss: 15.931
Iter: 0, Loss: 67.474, round_loss: 0.000, recon_loss: 67.474
Iter: 200, Loss: 749.161, round_loss: 699.600, recon_loss: 49.561
Iter: 400, Loss: 661.050, round_loss: 612.124, recon_loss: 48.926
Iter: 600, Loss: 579.686, round_loss: 534.517, recon_loss: 45.169
Iter: 800, Loss: 512.716, round_loss: 467.676, recon_loss: 45.040
Iter: 0, Loss: 21.089, round_loss: 0.000, recon_loss: 21.089
Iter: 200, Loss: 2197.208, round_loss: 2179.915, recon_loss: 17.293
Iter: 400, Loss: 1866.188, round_loss: 1850.508, recon_loss: 15.680
Iter: 600, Loss: 1513.389, round_loss: 1497.369, recon_loss: 16.020
Iter: 800, Loss: 1263.010, round_loss: 1247.208, recon_loss: 15.802
Iter: 0, Loss: 27.792, round_loss: 0.000, recon_loss: 27.792
Iter: 200, Loss: 2825.472, round_loss: 2800.985, recon_loss: 24.487
Iter: 400, Loss: 2392.405, round_loss: 2369.150, recon_loss: 23.255
Iter: 600, Loss: 1913.531, round_loss: 1890.370, recon_loss: 23.162
Iter: 800, Loss: 1581.791, round_loss: 1558.943, recon_loss: 22.849
Iter: 0, Loss: 11.405, round_loss: 0.000, recon_loss: 11.405
Iter: 200, Loss: 8732.586, round_loss: 8723.361, recon_loss: 9.224
Iter: 400, Loss: 7324.591, round_loss: 7315.312, recon_loss: 9.279
Iter: 600, Loss: 5558.384, round_loss: 5549.219, recon_loss: 9.165
Iter: 800, Loss: 4452.402, round_loss: 4443.282, recon_loss: 9.120
Iter: 0, Loss: 12.180, round_loss: 0.000, recon_loss: 12.180
Iter: 200, Loss: 11225.738, round_loss: 11214.384, recon_loss: 11.355
Iter: 400, Loss: 9402.935, round_loss: 9391.502, recon_loss: 11.432
Iter: 600, Loss: 7042.606, round_loss: 7031.720, recon_loss: 10.886
Iter: 800, Loss: 5588.718, round_loss: 5577.867, recon_loss: 10.851
Iter: 0, Loss: 12.454, round_loss: 0.000, recon_loss: 12.454
Iter: 200, Loss: 34924.961, round_loss: 34913.793, recon_loss: 11.169
Iter: 400, Loss: 29211.406, round_loss: 29201.236, recon_loss: 10.169
Iter: 600, Loss: 21638.107, round_loss: 21627.727, recon_loss: 10.381
Iter: 800, Loss: 17085.244, round_loss: 17074.867, recon_loss: 10.378
Iter: 0, Loss: 775.307, round_loss: 0.000, recon_loss: 775.307
Iter: 200, Loss: 45608.180, round_loss: 44893.336, recon_loss: 714.846
Iter: 400, Loss: 39685.809, round_loss: 39048.641, recon_loss: 637.169
Iter: 600, Loss: 34637.645, round_loss: 34020.820, recon_loss: 616.823
Iter: 800, Loss: 30633.617, round_loss: 29978.488, recon_loss: 655.129
Iter: 0, Loss: 0.643, round_loss: 0.000, recon_loss: 0.643
Iter: 200, Loss: 4864.605, round_loss: 4863.940, recon_loss: 0.665
Iter: 400, Loss: 4042.245, round_loss: 4041.592, recon_loss: 0.653
Iter: 600, Loss: 2669.002, round_loss: 2668.361, recon_loss: 0.641
Iter: 800, Loss: 1845.130, round_loss: 1844.454, recon_loss: 0.676
Completed block reconstruction
Block reconstruction took 154.96023273468018 seconds
Starting model evaluation of W3A3 block reconstruction (low)...
Top-1 accuracy: 63.24, Top-5 accuracy: 85.06
Starting model evaluation of W3A6 block reconstruction (med)...
Top-1 accuracy: 64.92, Top-5 accuracy: 86.19
Starting model evaluation of W3A8 block reconstruction (high)...
Top-1 accuracy: 64.86, Top-5 accuracy: 86.09
